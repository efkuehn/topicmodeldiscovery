{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding The Topics in HathiTrust Data \n",
    "\n",
    "To apply the topic model, we need to import it and the `corpus_dict` again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.ldamodel import LdaModel \n",
    "from gensim.corpora.dictionary import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LdaModel.load('./models/PrelimTopicModel2') \n",
    "corpus_dict = Dictionary.load_from_text('./models/corpus_dictionary_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also import the text cleaning resources from `pool_processing.py`. These are the same text cleaning resources we used for creating the topic model in `create_topic_models.ipynb`. `pool_process.py` is the script used to iterate over the entire Political Theology corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pool_processing import STOPWORDS\n",
    "from pool_processing import PUNCDIG_TRANSLATOR\n",
    "from pool_processing import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thankfully, HathiTrust as created a really helpful tool for feature extraction the [htrc-feature-reader](https://github.com/htrc/htrc-feature-reader) provides tools for accessing the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from htrc_features import FeatureReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get test files, to explore how the feature reader works, and how the LDA model works with some of the files in our Political Theology corpus, we can use the feature reader to download files directly from HathiTrust. This method will be too slow when we iterate over the entire corpus, but this will work as we explore the corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr = FeatureReader(ids=['chi.19073766'])\n",
    "# other option: uc1.32106018820081"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code produces a list of the volume objects collected from HathiTrust. These volumes can the be explored to see what is in the volume objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "volumes = [vol for vol in fr.volumes()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chi.19073766  |  The French Revolution : a political history, 1789-1804 / by A. Aulard ; translated from the French of the 3rd ed., with a preface, notes, and historical summary by Bernard Miall.\n"
     ]
    }
   ],
   "source": [
    "print(volumes[0].id, ' | ', volumes[0].title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell cleans the volume list according to the functions imported from `pool_process`, the data from HathiTrust is in a Pandas Dataframe. Because these words appear as feature extractions, we need to multiply their count based on how many times that word appears in the process. Also, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_list = []\n",
    "for vol in volumes:\n",
    "    for page in vol.pages():\n",
    "        df = page.tokenlist('body', case=False, pos=False)\n",
    "        dicty = df.to_dict()\n",
    "        count = dicty['count']\n",
    "        clean_list = []\n",
    "        for key in count.keys():\n",
    "            w = key[2]\n",
    "            if w not in STOPWORDS and len(w) > 2: # removing two character words\n",
    "                # The Translator should have been placed before the if test\n",
    "                w = w.translate(PUNCDIG_TRANSLATOR)\n",
    "                if w != '':\n",
    "                    clean_list += [lemmatizer.lemmatize(w)] * count[key]\n",
    "        vol_list.append(clean_list)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what one of the pages looks like after it has been cleaned. Notice these are just collection of the words. Many of them are errors because of OCR problems. This is one advantage to LDA topic modeling, it can be forgiving of errors as long as their is enough text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['erberhvth',\n",
       " 'th',\n",
       " 'brissot',\n",
       " 'admitted',\n",
       " 'afﬁliated',\n",
       " 'arcis',\n",
       " 'ary',\n",
       " 'assemble',\n",
       " 'assembly',\n",
       " 'attempted',\n",
       " 'bayonne',\n",
       " 'body',\n",
       " 'bordeaux',\n",
       " 'built',\n",
       " 'canton',\n",
       " 'cavalry',\n",
       " 'central',\n",
       " 'ceux',\n",
       " 'chalons',\n",
       " 'church',\n",
       " 'citizen',\n",
       " 'club',\n",
       " 'club',\n",
       " 'complaint',\n",
       " 'composed',\n",
       " 'conquered',\n",
       " 'contrary',\n",
       " 'convention',\n",
       " 'convert',\n",
       " 'cordeliers',\n",
       " 'danton',\n",
       " 'day',\n",
       " 'deliberation',\n",
       " 'demanded',\n",
       " 'demonstration',\n",
       " 'department',\n",
       " 'department',\n",
       " 'department',\n",
       " 'deputation',\n",
       " 'deputation',\n",
       " 'deputation',\n",
       " 'deputation',\n",
       " 'detach',\n",
       " 'devol',\n",
       " 'did',\n",
       " 'direction',\n",
       " 'défendront',\n",
       " 'eightyfour',\n",
       " 'eightyfour',\n",
       " 'electoral',\n",
       " 'entire',\n",
       " 'expelled',\n",
       " 'fact',\n",
       " 'far',\n",
       " 'federal',\n",
       " 'federal',\n",
       " 'federal',\n",
       " 'federal',\n",
       " 'federal',\n",
       " 'federal',\n",
       " 'federative',\n",
       " 'following',\n",
       " 'fortyeight',\n",
       " 'french',\n",
       " 'friend',\n",
       " 'gallery',\n",
       " 'garrison',\n",
       " 'gireydupré',\n",
       " 'girondist',\n",
       " 'girondist',\n",
       " 'guard',\n",
       " 'gué',\n",
       " 'hall',\n",
       " 'i',\n",
       " 'invitation',\n",
       " 'jacobin',\n",
       " 'jacobin',\n",
       " 'janu—',\n",
       " 'lanthenas',\n",
       " 'later',\n",
       " 'le',\n",
       " 'liberty',\n",
       " 'lisieux',\n",
       " 'lorient',\n",
       " 'majority',\n",
       " 'man',\n",
       " 'marat',\n",
       " 'marseillais',\n",
       " 'mean',\n",
       " 'member',\n",
       " 'met',\n",
       " 'miscarried',\n",
       " 'morning',\n",
       " 'mountain',\n",
       " 'movement',\n",
       " 'nantes',\n",
       " 'obtained',\n",
       " 'october',\n",
       " 'ouvet',\n",
       " 'paris',\n",
       " 'parisian',\n",
       " 'pelled',\n",
       " 'people',\n",
       " 'permission',\n",
       " 'perpignan',\n",
       " 'policy',\n",
       " 'qui',\n",
       " 'ran',\n",
       " 'receive',\n",
       " 'refrain',\n",
       " 'representative',\n",
       " 'representing',\n",
       " 'riom',\n",
       " 'robespierre',\n",
       " 'roland',\n",
       " 'saintbon',\n",
       " 'secession',\n",
       " 'secession',\n",
       " 'section',\n",
       " 'section',\n",
       " 'share',\n",
       " 'sixteen',\n",
       " 'society',\n",
       " 'society',\n",
       " 'song',\n",
       " 'soon',\n",
       " 'specially',\n",
       " 'spend',\n",
       " 'street',\n",
       " 'striking',\n",
       " 'swore',\n",
       " 'threatening',\n",
       " 'threat',\n",
       " 'tous',\n",
       " 'téte',\n",
       " 'undertook',\n",
       " 'use',\n",
       " 'valognes',\n",
       " 'were']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vol_list[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the text is cleaned in needs to be turned into the list of vectors that can be used to run the topic model against. This is where the `corpus_dict` comes in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_corpus = [corpus_dict.doc2bow(text) for text in vol_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The particular vectors for a given page can be explored in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = lda_model[other_corpus[100]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The zeroth item of the vector shows the closest topic matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 0.34206182), (6, 0.15319963), (12, 0.37902364)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we run the full text corpus through the topic model, we can see which pages match best to the topics defined in our test corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pot_match = []\n",
    "for doc_num, doc in enumerate(other_corpus):\n",
    "    vector = lda_model[doc]\n",
    "    row = sorted(vector[0], key=lambda x: x[1], reverse=True)\n",
    "    topic_num, prop_topic = row[0]\n",
    "    # notice, we are filtering on only the topics we think are   \n",
    "    # interesting and most coherent.\n",
    "    if topic_num in (0, 1, 3, 5, 6, 11):\n",
    "        pot_match.append((doc_num, topic_num, prop_topic))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of potential matches can be sorted based on the percentage given by the model with the follwing line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(375, 5, 0.8198943),\n",
       " (154, 5, 0.8079995),\n",
       " (24, 1, 0.8039998),\n",
       " (115, 5, 0.7599994),\n",
       " (227, 5, 0.7599993),\n",
       " (393, 1, 0.7599976),\n",
       " (121, 5, 0.75499904),\n",
       " (374, 5, 0.7309115),\n",
       " (208, 5, 0.67999905),\n",
       " (67, 5, 0.66688335),\n",
       " (213, 5, 0.65270615),\n",
       " (79, 5, 0.6517023),\n",
       " (232, 6, 0.6106287),\n",
       " (228, 5, 0.5843488),\n",
       " (185, 11, 0.577145),\n",
       " (103, 5, 0.5682204),\n",
       " (186, 11, 0.5482592),\n",
       " (42, 5, 0.5455658),\n",
       " (129, 5, 0.5329784),\n",
       " (174, 5, 0.5318797),\n",
       " (8, 5, 0.51999915),\n",
       " (359, 5, 0.51999915),\n",
       " (236, 5, 0.5180849),\n",
       " (328, 1, 0.5100005),\n",
       " (139, 1, 0.50999945),\n",
       " (166, 11, 0.50500137),\n",
       " (62, 5, 0.50333136),\n",
       " (263, 11, 0.50250286),\n",
       " (100, 5, 0.49848774),\n",
       " (183, 11, 0.49669254),\n",
       " (145, 5, 0.4828389),\n",
       " (268, 6, 0.460285),\n",
       " (246, 5, 0.45520547),\n",
       " (184, 11, 0.43896687),\n",
       " (9, 11, 0.43428633),\n",
       " (46, 11, 0.42833412),\n",
       " (229, 5, 0.4201152),\n",
       " (314, 5, 0.4197382),\n",
       " (10, 6, 0.4171695),\n",
       " (136, 5, 0.41648477),\n",
       " (182, 11, 0.4160481),\n",
       " (164, 11, 0.41353098),\n",
       " (81, 11, 0.40800238),\n",
       " (270, 11, 0.40400216),\n",
       " (255, 6, 0.4007121),\n",
       " (171, 11, 0.399741),\n",
       " (322, 5, 0.3915353),\n",
       " (385, 5, 0.39085233),\n",
       " (134, 5, 0.38020995),\n",
       " (283, 5, 0.36365366),\n",
       " (271, 6, 0.36308786),\n",
       " (53, 5, 0.35905674),\n",
       " (49, 11, 0.34905064),\n",
       " (65, 5, 0.3481434),\n",
       " (34, 5, 0.3466663),\n",
       " (188, 11, 0.34500867),\n",
       " (261, 6, 0.34464112),\n",
       " (262, 11, 0.34462065),\n",
       " (326, 5, 0.34362164),\n",
       " (330, 5, 0.34173334),\n",
       " (192, 11, 0.34000042),\n",
       " (325, 3, 0.33778),\n",
       " (52, 5, 0.33777723),\n",
       " (159, 6, 0.33389673),\n",
       " (161, 3, 0.33381954),\n",
       " (64, 5, 0.3328618),\n",
       " (155, 11, 0.3301619),\n",
       " (175, 6, 0.32662037),\n",
       " (128, 11, 0.32293704),\n",
       " (187, 11, 0.31517765),\n",
       " (388, 5, 0.31238306),\n",
       " (390, 5, 0.3090886),\n",
       " (247, 5, 0.30744436),\n",
       " (269, 5, 0.30293378),\n",
       " (265, 6, 0.29570827),\n",
       " (48, 11, 0.2870301),\n",
       " (258, 11, 0.27589446),\n",
       " (160, 5, 0.27569914),\n",
       " (140, 5, 0.27295923),\n",
       " (97, 3, 0.26865536),\n",
       " (294, 5, 0.26687807),\n",
       " (303, 1, 0.25153613),\n",
       " (16, 6, 0.24684039),\n",
       " (20, 11, 0.20762493),\n",
       " (63, 0, 0.19941553),\n",
       " (0, 0, 0.04),\n",
       " (1, 0, 0.04),\n",
       " (2, 0, 0.04),\n",
       " (3, 0, 0.04),\n",
       " (4, 0, 0.04),\n",
       " (5, 0, 0.04),\n",
       " (7, 0, 0.04),\n",
       " (11, 0, 0.04),\n",
       " (12, 0, 0.04),\n",
       " (13, 0, 0.04),\n",
       " (29, 0, 0.04),\n",
       " (44, 0, 0.04),\n",
       " (153, 0, 0.04),\n",
       " (357, 0, 0.04),\n",
       " (363, 0, 0.04),\n",
       " (396, 0, 0.04),\n",
       " (397, 0, 0.04),\n",
       " (398, 0, 0.04),\n",
       " (399, 0, 0.04),\n",
       " (400, 0, 0.04),\n",
       " (401, 0, 0.04)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(pot_match, key=lambda x: x[-1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we still have vol_list defined as one particular volume, you can compare what words were fed to the model to produce the above results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['th',\n",
       " 'th',\n",
       " 'th',\n",
       " 'rd',\n",
       " 'th',\n",
       " 'th',\n",
       " 'th',\n",
       " 'tribunal',\n",
       " 'accepted',\n",
       " 'adjourned',\n",
       " 'alternative',\n",
       " 'appeared',\n",
       " 'article',\n",
       " 'article',\n",
       " 'assemblage',\n",
       " 'assemble',\n",
       " 'assemble',\n",
       " 'club',\n",
       " 'commune',\n",
       " 'commune',\n",
       " 'condemned',\n",
       " 'conformably',\n",
       " 'constitution',\n",
       " 'contrary',\n",
       " 'correctional',\n",
       " 'court',\n",
       " 'day',\n",
       " 'debated',\n",
       " 'delivered',\n",
       " 'dissolved',\n",
       " 'duplantier',\n",
       " 'election',\n",
       " 'following',\n",
       " 'following',\n",
       " 'franc',\n",
       " 'french',\n",
       " 'fructidor',\n",
       " 'general',\n",
       " 'general',\n",
       " 'germinal',\n",
       " 'germinal',\n",
       " 'germinal',\n",
       " 'guilty',\n",
       " 'heard',\n",
       " 'imprisonment',\n",
       " 'increased',\n",
       " 'individual',\n",
       " 'inhabitant',\n",
       " 'inhabitant',\n",
       " 'law',\n",
       " 'law',\n",
       " 'law',\n",
       " 'law',\n",
       " 'law',\n",
       " 'led',\n",
       " 'mailhe',\n",
       " 'mailhes',\n",
       " 'majority',\n",
       " 'measure',\n",
       " 'measure',\n",
       " 'meet',\n",
       " 'meeting',\n",
       " 'member',\n",
       " 'member',\n",
       " 'messidor',\n",
       " 'month',\n",
       " 'month',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'motion',\n",
       " 'new',\n",
       " 'number',\n",
       " 'occupying',\n",
       " 'occupying',\n",
       " 'order',\n",
       " 'order',\n",
       " 'pay',\n",
       " 'people',\n",
       " 'police',\n",
       " 'policecourts',\n",
       " 'political',\n",
       " 'political',\n",
       " 'political',\n",
       " 'premise',\n",
       " 'present',\n",
       " 'prevent',\n",
       " 'principal',\n",
       " 'principle',\n",
       " 'principle',\n",
       " 'private',\n",
       " 'private',\n",
       " 'professed',\n",
       " 'professed',\n",
       " 'prohibited',\n",
       " 'prohibition',\n",
       " 'proposal',\n",
       " 'proposed',\n",
       " 'proposed',\n",
       " 'proprietor',\n",
       " 'proscribe',\n",
       " 'prosecuted',\n",
       " 'provisionally',\n",
       " 'punished',\n",
       " 'punishment',\n",
       " 'question',\n",
       " 'question',\n",
       " 'receive',\n",
       " 'reconstitution',\n",
       " 'relating',\n",
       " 'repealed',\n",
       " 'replaced',\n",
       " 'report',\n",
       " 'report',\n",
       " 'respectively',\n",
       " 'restrictive',\n",
       " 'resumed',\n",
       " 'revolutionary',\n",
       " 'ridiculous',\n",
       " 'riotous',\n",
       " 'rise',\n",
       " 'severity',\n",
       " 'shall',\n",
       " 'shall',\n",
       " 'shall',\n",
       " 'shall',\n",
       " 'shall',\n",
       " 'shall',\n",
       " 'simpler',\n",
       " 'society',\n",
       " 'society',\n",
       " 'society',\n",
       " 'society',\n",
       " 'society',\n",
       " 'society',\n",
       " 'subject',\n",
       " 'sunset',\n",
       " 'tenant',\n",
       " 'terminate',\n",
       " 'thermidor',\n",
       " 'thermidor',\n",
       " 'thermidor',\n",
       " 'time',\n",
       " 'twice',\n",
       " 'undergo',\n",
       " 'vaublanc',\n",
       " 'worded',\n",
       " 'year',\n",
       " 'year',\n",
       " 'year',\n",
       " 'the',\n",
       " 'ﬁne']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vol_list[375]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matches below .04 are not worth concedering. The match is to low to be relevant, so we can filter these matches out with the following list comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_sort = [x for x in pot_match if x[-1] > .04]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the shortened list, we can easily create a pandas Dataframe and do various operiations to find out more about the book as a whole. In the first example, we found the average rating for the topics in the book. This calculates the average rating everytime a given topic is dominant on a page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topic_num\n",
       "0     0.199416\n",
       "1     0.567107\n",
       "3     0.313418\n",
       "5     0.483392\n",
       "6     0.379959\n",
       "11    0.395218\n",
       "Name: perc, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_df = pd.DataFrame(short_sort, columns=['page', 'topic_num', 'perc'])\n",
    "sorted_df.groupby(['topic_num'])['perc'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sections adds `mean` as a column in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_sorted_df = pd.DataFrame(short_sort, columns=['page', 'topic_num', 'perc'])\n",
    "short_sorted_df['mean'] = short_sorted_df.groupby('topic_num')['perc'].transform('mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code adds the count of how many times a given topic number is dominant on a page. This would help to calculate how much of the book is about a given topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_sorted_df['count'] = short_sorted_df.groupby(['topic_num'])['topic_num'].transform('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `count` as a column, we can see that topic number 5 is the most prominant topic in the book, having the best match, the highest average, and on the most pages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page</th>\n",
       "      <th>perc</th>\n",
       "      <th>mean</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_num</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>0.199416</td>\n",
       "      <td>0.199416</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>393</td>\n",
       "      <td>0.804000</td>\n",
       "      <td>0.567107</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>325</td>\n",
       "      <td>0.337780</td>\n",
       "      <td>0.313418</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>390</td>\n",
       "      <td>0.819894</td>\n",
       "      <td>0.483392</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>271</td>\n",
       "      <td>0.610629</td>\n",
       "      <td>0.379959</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>270</td>\n",
       "      <td>0.577145</td>\n",
       "      <td>0.395218</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           page      perc      mean  count\n",
       "topic_num                                 \n",
       "0            63  0.199416  0.199416      1\n",
       "1           393  0.804000  0.567107      5\n",
       "3           325  0.337780  0.313418      3\n",
       "5           390  0.819894  0.483392     43\n",
       "6           271  0.610629  0.379959     10\n",
       "11          270  0.577145  0.395218     23"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_sorted_df.groupby(['topic_num']).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what the dataframe looks like now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page</th>\n",
       "      <th>topic_num</th>\n",
       "      <th>perc</th>\n",
       "      <th>mean</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.519999</td>\n",
       "      <td>0.483392</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>0.434286</td>\n",
       "      <td>0.395218</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>0.417170</td>\n",
       "      <td>0.379959</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>0.246840</td>\n",
       "      <td>0.379959</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>11</td>\n",
       "      <td>0.207625</td>\n",
       "      <td>0.395218</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0.804000</td>\n",
       "      <td>0.567107</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>34</td>\n",
       "      <td>5</td>\n",
       "      <td>0.346666</td>\n",
       "      <td>0.483392</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>42</td>\n",
       "      <td>5</td>\n",
       "      <td>0.545566</td>\n",
       "      <td>0.483392</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>46</td>\n",
       "      <td>11</td>\n",
       "      <td>0.428334</td>\n",
       "      <td>0.395218</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>48</td>\n",
       "      <td>11</td>\n",
       "      <td>0.287030</td>\n",
       "      <td>0.395218</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>49</td>\n",
       "      <td>11</td>\n",
       "      <td>0.349051</td>\n",
       "      <td>0.395218</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>52</td>\n",
       "      <td>5</td>\n",
       "      <td>0.337777</td>\n",
       "      <td>0.483392</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>53</td>\n",
       "      <td>5</td>\n",
       "      <td>0.359057</td>\n",
       "      <td>0.483392</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>62</td>\n",
       "      <td>5</td>\n",
       "      <td>0.503331</td>\n",
       "      <td>0.483392</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>0.199416</td>\n",
       "      <td>0.199416</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    page  topic_num      perc      mean  count\n",
       "0      8          5  0.519999  0.483392     43\n",
       "1      9         11  0.434286  0.395218     23\n",
       "2     10          6  0.417170  0.379959     10\n",
       "3     16          6  0.246840  0.379959     10\n",
       "4     20         11  0.207625  0.395218     23\n",
       "5     24          1  0.804000  0.567107      5\n",
       "6     34          5  0.346666  0.483392     43\n",
       "7     42          5  0.545566  0.483392     43\n",
       "8     46         11  0.428334  0.395218     23\n",
       "9     48         11  0.287030  0.395218     23\n",
       "10    49         11  0.349051  0.395218     23\n",
       "11    52          5  0.337777  0.483392     43\n",
       "12    53          5  0.359057  0.483392     43\n",
       "13    62          5  0.503331  0.483392     43\n",
       "14    63          0  0.199416  0.199416      1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_sorted_df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running over the Corpus \n",
    "\n",
    "To run over the entire Political Theology corpus, we can simplify the above explorations into a few functions. These functions form the bases for the `pool_process.py` script that was used in the creation of our data set. \n",
    "\n",
    "The doc strings in the following functions explain what the code does. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Topic = namedtuple('Topic', ['top_num', 'perc'])\n",
    "BestMatch = namedtuple('BestMatch', ['page', 'top_num', 'perc'])\n",
    "Book = namedtuple('Book', ['ht_id', 'top_topic', 'best_match', 'most_common_topic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(lst):\n",
    "    '''This takes an average of a python list. Numpy Arrays have a built in method for this.'''\n",
    "    return sum(lst) / len(lst)\n",
    "\n",
    "\n",
    "def volume_parser(vol):\n",
    "    '''Clean the body of a HathiTrust volume. This runs the Wordnet Lemmatizer'''\n",
    "    vol_list = []\n",
    "    for page in vol.pages():\n",
    "        df = page.tokenlist('body', case=False, pos=False)\n",
    "        dicty = df.to_dict()\n",
    "        count = dicty['count']\n",
    "        clean_list = []\n",
    "        for key in count.keys():\n",
    "            w = key[2]\n",
    "            if w not in STOPWORDS and len(w) > 2: # removing two character words\n",
    "                w = w.translate(PUNCDIG_TRANSLATOR)\n",
    "                # The PUNCDIG_TRANSLATOR should have been placed before the test\n",
    "                if w != '':\n",
    "                    clean_list += [lemmatizer.lemmatize(w)] * count[key]\n",
    "        vol_list.append(clean_list)\n",
    "    return vol_list\n",
    "\n",
    "\n",
    "def get_topic_average(sorted_list):\n",
    "    '''This averages the topics in a list of pages, topics, and percentages'''\n",
    "    dicty = {}\n",
    "    for (_, x, y) in sorted_list:\n",
    "        dicty.setdefault(x, []).append(y)\n",
    "    topic_averages = [(x, mean(y)) for x, y in dicty.items()]\n",
    "    return topic_averages\n",
    "\n",
    "        \n",
    "def analyze_corpus_with_model(other_coprus, lda_model):\n",
    "    '''Filters an unseen corpus on the original LDA Model'''\n",
    "    pot_match = []\n",
    "    for doc_num, doc in enumerate(other_corpus):\n",
    "        vector = lda_model[doc]\n",
    "        # row = sorted(vector[0], key=lambda x: x[1], reverse=True)\n",
    "        # topic_num, prop_topic = row[0]\n",
    "        topic_num, prop_topic = max(vector[0], key=lambda x: x[1])\n",
    "        if topic_num in (0, 1, 3, 5, 6, 11) and prop_topic > .04:\n",
    "            pot_match.append((doc_num, topic_num, prop_topic))\n",
    "    return pot_match \n",
    "    \n",
    "    \n",
    "def corpus_parser(corpus_list, corpus_dict, ldamodel):\n",
    "    '''This takes a new corpus and return the best matches, \n",
    "    the most common topics and the top topics in a new corpus. \n",
    "    `corpus_list` needs to be a list of lists of the words on a page'''\n",
    "    other_corpus = [corpus_dict.doc2bow(text) for text in vol_list]\n",
    "    sorted_list = analyze_corpus_with_model(other_corpus, ldamodel)\n",
    "    best_match = max(sorted_list, key=lambda x: x[-1])\n",
    "    most_common_topic = Counter([x[1] for x in sorted_list]).most_common(1).pop()\n",
    "    top_topic = max(get_topic_average(sorted_list), key=lambda x: x[-1])\n",
    "    return best_match, most_common_topic, top_topic\n",
    "    \n",
    "\n",
    "def file_parser(feature_reader, corpus_dict, analyzed_dict, ldamodel):\n",
    "    '''returns a dictionary of each volume, and the topics represented\n",
    "    in that volume.'''\n",
    "    for vol in feature_reader.volumes():\n",
    "        corpus_list = volume_parser(vol)\n",
    "        best_match, most_common_topic, top_topic = corpus_parser(corpus_list, corpus_dict, ldamodel)\n",
    "        analyzed_dict[vol.id] = Book(vol.id, Topic(*top_topic), BestMatch(*best_match), Topic(*most_common_topic))\n",
    "    return analyzed_dict  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try this code on the following 10 items. But making these calls to HathiTrust will take a few minutes to run over all of the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['chi.086834843',\n",
    " 'chi.096807539',\n",
    " 'chi.098001406',\n",
    " 'chi.100957606',\n",
    " 'chi.19073766',\n",
    " 'chi.096733853',\n",
    " 'chi.098001359',\n",
    " 'chi.098383507',\n",
    " 'chi.11963941',\n",
    " 'chi.19080474'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr = FeatureReader(ids=files)\n",
    "analyzed_dict = file_parser(fr, corpus_dict, {}, lda_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the results of the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chi.086834843': Book(ht_id='chi.086834843', top_topic=Topic(top_num=1, perc=0.5824437936147054), best_match=BestMatch(page=375, top_num=5, perc=0.81989264), most_common_topic=Topic(top_num=5, perc=47)),\n",
       " 'chi.096807539': Book(ht_id='chi.096807539', top_topic=Topic(top_num=1, perc=0.5351911102022443), best_match=BestMatch(page=375, top_num=5, perc=0.81989455), most_common_topic=Topic(top_num=5, perc=44)),\n",
       " 'chi.098001406': Book(ht_id='chi.098001406', top_topic=Topic(top_num=1, perc=0.5570442179838816), best_match=BestMatch(page=375, top_num=5, perc=0.8198953), most_common_topic=Topic(top_num=5, perc=44)),\n",
       " 'chi.100957606': Book(ht_id='chi.100957606', top_topic=Topic(top_num=1, perc=0.5560638954242071), best_match=BestMatch(page=375, top_num=5, perc=0.81989455), most_common_topic=Topic(top_num=5, perc=43)),\n",
       " 'chi.19073766': Book(ht_id='chi.19073766', top_topic=Topic(top_num=1, perc=0.6181326270103454), best_match=BestMatch(page=375, top_num=5, perc=0.8198934), most_common_topic=Topic(top_num=5, perc=47)),\n",
       " 'chi.096733853': Book(ht_id='chi.096733853', top_topic=Topic(top_num=1, perc=0.645752027630806), best_match=BestMatch(page=375, top_num=5, perc=0.8198932), most_common_topic=Topic(top_num=5, perc=45)),\n",
       " 'chi.098001359': Book(ht_id='chi.098001359', top_topic=Topic(top_num=1, perc=0.5671495258808136), best_match=BestMatch(page=375, top_num=5, perc=0.81989396), most_common_topic=Topic(top_num=5, perc=44)),\n",
       " 'chi.098383507': Book(ht_id='chi.098383507', top_topic=Topic(top_num=1, perc=0.5658926725387573), best_match=BestMatch(page=375, top_num=5, perc=0.81989247), most_common_topic=Topic(top_num=5, perc=44)),\n",
       " 'chi.11963941': Book(ht_id='chi.11963941', top_topic=Topic(top_num=1, perc=0.5824437886476517), best_match=BestMatch(page=375, top_num=5, perc=0.8198917), most_common_topic=Topic(top_num=5, perc=46)),\n",
       " 'chi.19080474': Book(ht_id='chi.19080474', top_topic=Topic(top_num=1, perc=0.6459993571043015), best_match=BestMatch(page=375, top_num=5, perc=0.8198934), most_common_topic=Topic(top_num=5, perc=46))}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzed_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Performance \n",
    "\n",
    "The performance of this code is a little dissapointing. Part of this is because NLTK has not been optimized for performance. SpaCy would have provided a faster way of Lemmantizing. But also LDA is a costly compute. To explore how long the code actually takes to run, and to see where the cost hits are, the following two sections use `timeit` and `cProfile`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timeit\n",
    "import timeit\n",
    "def wrapper(func, *args, **kwargs):\n",
    "    def wrapped():\n",
    "        return func(*args, **kwargs)\n",
    "    return wrapped\n",
    "\n",
    "wrapped = wrapper(file_parser, fr, corpus_dict, {}, lda_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile \n",
    "cProfile.run('file_parser(fr, corpus_dict {}, lda_model)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit.timeit(wrapped, number=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
