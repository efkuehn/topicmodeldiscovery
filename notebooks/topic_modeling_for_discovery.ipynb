{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding The Topics in HathiTrust Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from htrc_features import FeatureReader\n",
    "import glob \n",
    "# import modules reused from the creation of the topic model\n",
    "from pool_processing_test import STOPWORDS\n",
    "from pool_processing_test import PUNCDIG_TRANSLATOR\n",
    "from pool_processing_test import WordNetLemmatizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = glob.glob('../data/testfiles/*.bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr = FeatureReader(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vol_gen(feature_reader):\n",
    "    for vol in feature_reader.volumes():\n",
    "        yield vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volues = vol_gen(fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol = next(volues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_list = []\n",
    "for vol in fr.volumes():\n",
    "    for page in vol.pages():\n",
    "        df = page.tokenlist('body', case=False, pos=False)\n",
    "        dicty = df.to_dict()\n",
    "        count = dicty['count']\n",
    "        clean_list = []\n",
    "        for key in count.keys():\n",
    "            w = key[2]\n",
    "            if w not in STOPWORDS and len(w) > 2: # removing two character words\n",
    "                w = w.translate(PUNCDIG_TRANSLATOR)\n",
    "                if w != '':\n",
    "                    clean_list += [lemmatizer.lemmatize(w)] * count[key]\n",
    "                    # clean_list.append(lemmatizer.lemmatize(w) * count[key])\n",
    "        vol_list.append(clean_list)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[lemmatizer.lemmatize(\"apparatus\")] * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_list[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_corpus = [corpus_dict.doc2bow(text) for text in vol_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = lda_model[other_corpus[100]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = sorted(vector[0], key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " for j, (topic_num, prop_topic) in enumerate(row):\n",
    "        print(j, topic_num, prop_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pot_match = []\n",
    "for doc_num, doc in enumerate(other_corpus):\n",
    "    vector = lda_model[doc]\n",
    "    row = sorted(vector[0], key=lambda x: x[1], reverse=True)\n",
    "    topic_num, prop_topic = row[0]\n",
    "    if topic_num in (0, 1, 3, 5, 6, 11):\n",
    "        pot_match.append((doc_num, topic_num, prop_topic))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(pot_match, key=lambda x: x[-1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_list[482]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a list of tuples: page, topic_num, percentage\n",
    "sorted_list = sorted(pot_match, key=lambda x: x[-1], reverse=True)\n",
    "sorted_list[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_sort = [x for x in sorted_list if x[-1] > .04]\n",
    "short_sort[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(short_sort))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df = pd.DataFrame(sorted_list, columns=['page', 'topic_num', 'perc'])\n",
    "sorted_df.groupby(['topic_num']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_sorted_df = pd.DataFrame(short_sort, columns=['page', 'topic_num', 'perc'])\n",
    "short_sorted_df['mean'] = short_sorted_df.groupby('topic_num')['perc'].transform('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_sorted_df['count'] = short_sorted_df.groupby(['topic_num'])['topic_num'].transform('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_sorted_df.groupby(['topic_num']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_sorted_df[short_sorted_df['topic_num'] == 6].plot(y='perc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(short_sorted_df['topic_num']).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running over the Corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Topic = namedtuple('Topic', ['top_num', 'perc'])\n",
    "BestMatch = namedtuple('BestMatch', ['page', 'top_num', 'perc'])\n",
    "Book = namedtuple('Book', ['ht_id', 'top_topic', 'best_match', 'most_common_topic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(lst):\n",
    "    return sum(lst) / len(lst)\n",
    "\n",
    "\n",
    "def volume_parser(vol):\n",
    "    vol_list = []\n",
    "    for page in vol.pages():\n",
    "        df = page.tokenlist('body', case=False, pos=False)\n",
    "        dicty = df.to_dict()\n",
    "        count = dicty['count']\n",
    "        clean_list = []\n",
    "        for key in count.keys():\n",
    "            w = key[2]\n",
    "            if w not in STOPWORDS and len(w) > 2: # removing two character words\n",
    "                w = w.translate(PUNCDIG_TRANSLATOR)\n",
    "                if w != '':\n",
    "                    clean_list += [lemmatizer.lemmatize(w)] * count[key]\n",
    "                    # clean_list.append(lemmatizer.lemmatize(w) * count[key])\n",
    "        vol_list.append(clean_list)\n",
    "    return vol_list\n",
    "\n",
    "\n",
    "def get_topic_average(sorted_list):\n",
    "    dicty = {}\n",
    "    for (_, x, y) in sorted_list:\n",
    "        dicty.setdefault(x, []).append(y)\n",
    "    topic_averages = [(x, mean(y)) for x, y in dicty.items()]\n",
    "    return topic_averages\n",
    "\n",
    "        \n",
    "def analyze_corpus_with_model(other_coprus, lda_model):\n",
    "    pot_match = []\n",
    "    for doc_num, doc in enumerate(other_corpus):\n",
    "        vector = lda_model[doc]\n",
    "        # row = sorted(vector[0], key=lambda x: x[1], reverse=True)\n",
    "        # topic_num, prop_topic = row[0]\n",
    "        topic_num, prop_topic = max(vector[0], key=lambda x: x[1])\n",
    "        if topic_num in (0, 1, 3, 5, 6, 11) and prop_topic > .04:\n",
    "            pot_match.append((doc_num, topic_num, prop_topic))\n",
    "    return pot_match \n",
    "    '''\n",
    "    sorted_list = sorted(pot_match, key=lambda x: x[-1], reverse=True)\n",
    "    return sorted_list\n",
    "    '''\n",
    "    \n",
    "    \n",
    "def corpus_parser(corpus_list, corpus_dict, ldamodel):\n",
    "    other_corpus = [corpus_dict.doc2bow(text) for text in vol_list]\n",
    "    sorted_list = analyze_corpus_with_model(other_corpus, ldamodel)\n",
    "    best_match = max(sorted_list, key=lambda x: x[-1])\n",
    "    most_common_topic = Counter([x[1] for x in sorted_list]).most_common(1).pop()\n",
    "    top_topic = max(get_topic_average(sorted_list), key=lambda x: x[-1])\n",
    "    return best_match, most_common_topic, top_topic\n",
    "    \n",
    "\n",
    "def file_parser(feature_reader, corpus_dict, analyzed_dict, ldamodel):\n",
    "    for vol in feature_reader.volumes():\n",
    "        corpus_list = volume_parser(vol)\n",
    "        best_match, most_common_topic, top_topic = corpus_parser(corpus_list, corpus_dict, ldamodel)\n",
    "        analyzed_dict[vol.id] = Book(vol.id, Topic(*top_topic), BestMatch(*best_match), Topic(*most_common_topic))\n",
    "    return analyzed_dict\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('../data/test/*.bz2')\n",
    "fr = FeatureReader(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr = FeatureReader(files)\n",
    "analyzed_dict = file_parser(fr, corpus_dict, {}, lda_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzed_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timeit\n",
    "import timeit\n",
    "def wrapper(func, *args, **kwargs):\n",
    "    def wrapped():\n",
    "        return func(*args, **kwargs)\n",
    "    return wrapped\n",
    "\n",
    "wrapped = wrapper(file_parser, fr, corpus_dict, {}, lda_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile \n",
    "cProfile.run('file_parser(fr, {}, lda_model)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit.timeit(wrapped, number=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
