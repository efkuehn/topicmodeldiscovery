{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing PDFs \n",
    "\n",
    "Pull in all of the PDF files and create objects for the text inside each one. \n",
    "\n",
    "\n",
    "\n",
    "[One of the sources I am using for the topic modeling](https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/)\n",
    "\n",
    "[This is a good post on Lemmatizing in python](https://www.machinelearningplus.com/nlp/lemmatization-examples-python/)\n",
    "\n",
    "For the pdf manipulation we use `PyPDF2`. This allows the text to be easily extracted from the scanned pdfs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2 \n",
    "from glob import glob\n",
    "\n",
    "pdfs = glob('../pdfs/*.pdf') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Texts \n",
    "\n",
    "Because these texts are pdf scans a lot of clean up will need to go into the OCR. It would be better to add spell check and other things, but we will just be pulling out the stop words, the obvious misspellings, and misdivided words. We used the `WordNetLemmatize` for lemmatizing the words, and SciKit Learn's stopwords collection for english. \n",
    "\n",
    "The first time this is ran, you will need to download both `wordnet` and `punkt` for the NLTK libraries to work. \n",
    "\n",
    "The NLTK library is slow, another option would have been to use [SpaCy](https://spacy.io/) to do the tokenization and lemmatization. The slowness of NLTK really showed up when we ran this over the entire corpus. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizing and cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/sgoodwin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/sgoodwin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS \n",
    "\n",
    "ADD_PUNC = '”“’’–˙ˆ‘'\n",
    "STOPWORDS = {'d', 'c', 'e', 's', 'œ', 'dhs', 'hk', 'nagy', 'eology', 'ey', 'g', 'ing', 'tion', 'er', 'rst', 'vol', 'ed'} \n",
    "AUTHOR_NAMES = {'cruz', 'frederiks', 'nagy', 'snyder', 'nguyen', 'prior', 'cavanaugh', 'heyer', 'schmil', 'smith', 'groody', 'campese', 'izuzquiza', 'heimburger', 'myers', 'colwell', 'olofinjana', 'krabill', 'norton', 'theocharous', 'nacpil', 'nnamani', 'soares', 'thompson', 'zendher', 'ahn', 'haug', 'sarmiento', 'davidson', 'rowlands', 'strine', 'zink', 'jimenez'}\n",
    "STOPWORDS = STOPWORDS.union(AUTHOR_NAMES)\n",
    "STOPWORDS = STOPWORDS.union(ENGLISH_STOP_WORDS)\n",
    "PUNCDIG_TRANSLATOR = str.maketrans('', '', string.punctuation+string.digits+ADD_PUNC)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_clean(text):\n",
    "    '''This function uses NLTK to tokenize the text (into words) as well as \n",
    "    remove stop words, and lemmatize the words. So that `dogs` becomes `dog`.'''\n",
    "    clean_list = []\n",
    "    words = nltk.word_tokenize(text)\n",
    "    for w in words:\n",
    "        if w not in STOPWORDS and len(w) > 2: # removing two character words\n",
    "            w = w.translate(PUNCDIG_TRANSLATOR)\n",
    "            if w != '':\n",
    "                clean_list.append(lemmatizer.lemmatize(w))\n",
    "    return clean_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['striped', 'bat', 'hanging', 'foot', 'best']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clean(\"The striped bats are hanging on their feet3 for best.\".lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Texts\n",
    "\n",
    "PyPDf2 takes an open file object. The following cells show a couple of features of the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PdfReadWarning: Xref table not zero-indexed. ID numbers for objects will be corrected. [pdf.py:1736]\n"
     ]
    }
   ],
   "source": [
    "pdf = open(pdfs[0], 'rb')\n",
    "pdf_obj = PyPDF2.PdfFileReader(pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of pages: 19\n"
     ]
    }
   ],
   "source": [
    "print('No. of pages: {}'.format(pdf_obj.numPages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will be used to create two different lists of lists. One is a running list of the pages of the text, the second is a list of tuples with the file_name and page number for the given extract. This will allow look up of a particular page the corpus list is refering to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_extractor(pdf, corpus_list, text_list):\n",
    "    '''Extract the text of pdfs and return a dictionary with\n",
    "    the file name as a key, and the value being a list of the pages\n",
    "    and the containing texts\n",
    "    '''\n",
    "    pdf_file_obj = open(pdf, 'rb')\n",
    "    pdf_obj = PyPDF2.PdfFileReader(pdf_file_obj)\n",
    "    for pn in range(0,pdf_obj.numPages):\n",
    "        page = pdf_obj.getPage(pn)\n",
    "        text = page.extractText().lower()\n",
    "        cleaned_list = text_clean(text)\n",
    "        corpus_list.append(cleaned_list)\n",
    "        text_list.append((pdf, pn))\n",
    "        # if you want to create a dictionary\n",
    "        # text_dict.setdefault(pdf, []).append(page.extractText())\n",
    "    pdf_file_obj.close()\n",
    "    return corpus_list, text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_list = []\n",
    "text_list = []\n",
    "\n",
    "for pdf in pdfs:\n",
    "    corpus_list, text_list = pdf_extractor(pdf, corpus_list, text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating LDA Model \n",
    "\n",
    "[LDA Topic Models](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) are short for Latent Dirichlet allocation. It does have an advantages, but it isn't a deterministic algorithim like [NMF](https://en.wikipedia.org/wiki/Non-negative_matrix_factorization). This means, that everytime the follwoing code is run, it will produce a different model. \n",
    "\n",
    "In the future I think it would be helpful to look at using NMF models for this process. \n",
    "\n",
    "The form of the model that we are using is installed with gensim. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora \n",
    "from gensim.models.ldamodel import LdaModel "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of different perameters that can be tweaked inside a topic mdoel. One of the big points about topic modeling is the number of topics. Random_state gives the computer a starting point. Chunksize is how big a piece is taken in. Filter-extremes means that rare words and super common words are not used for topic consideration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_topic_model(corpus_list):\n",
    "    corpus_dict = corpora.Dictionary(corpus_list)\n",
    "    corpus_dict.filter_extremes(no_below=100, no_above=0.5)\n",
    "    corpus = [corpus_dict.doc2bow(text) for text in corpus_list]\n",
    "    lda_model = LdaModel(corpus=corpus, \n",
    "                        id2word=corpus_dict, num_topics=25,\n",
    "                        random_state=100, update_every=1,\n",
    "                        chunksize=100, passes=50,\n",
    "                        alpha='symmetric', per_word_topics=True)\n",
    "    return lda_model, corpus, corpus_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model, corpus, corpus_dict = prepare_topic_model(corpus_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "import json\n",
    "# After the model has been created save the model \n",
    "# lda_model.save('./models/PrelimTopicModel2')\n",
    "# corpus_dict.save_as_text('./models/corpus_dictionary_2')\n",
    "# with open('./models/corpus.json', 'w') as fp:\n",
    "#    json.dump(corpus, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
